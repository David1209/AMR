\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{xcolor}
% \usepackage[margin=1.7cm]{geometry}
\usepackage{colortbl}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{textcomp}
\newcommand{\todo}[2]{{\color{red}\textbf{Todo voor #1:} #2}}
\begin{document}
\begin{titlepage}
\begin{center}
    \includegraphics[width=\textwidth]{./logo.png}
    \\ [2.5cm]
    \textsc{\Large Autonomous Mobile Robots}
    \\ [0.5cm]
    \textsc{\large Fourth assignment}
    \\ [1cm]
    \hrule
    \vspace{0.3cm}
    \textsc{Particle Filter Based Simultaneous Localization And Mapping}
    \\ [0.3cm]
    \hrule
    \vfill
    \textsc{Ruben Janssen, 10252657 \\ David van Erkelens, 10264019 \\[0.7cm] Department of Computer Science \\ University of Amsterdam \\[0.3cm] \today \\[0.5cm] All code published on}
    \\
    \verb|www.github.com/David1209/AMR/assignment4|
\end{center}
\end{titlepage}
\tableofcontents
\clearpage
\section{Introduction}
In order for an autonomous mobile robot to be able to navigate without a priori knowledge about the map, it has to determine its position whilst building the map. This can be accomplished by using the Simultaneous Localization and Mapping (SLAM) algorithm. One of the variations of SLAM is FastSLAM, a particle filter based algorithm which has a better performance in real time map building than most other variations. FastSLAM scales logarithmically with the number of landmarks in the map, and is therefore much more suitable for real-time applications.

\section{Setup}
A NXT robot has been equipped with an omnidirectional camera. Using this NXT, a dataset of images has been made instead of a live video feed. Eventhough using the dataset is less accurate, the FastSLAM algorithm should be able to compensate for this. A basic environment has been taped out, and from the point of view of the robot some snapshots have been made. 

insert picture of environment here

iets uitgebreider? wat?

\section{FastSLAM}
To implement FastSLAM, a pre-recorded dataset is used. The input of the SLAM algorithm consists of odometry data and observable features. Here, corners are used as landmarks, detected as endpoint segments of lines extracted from data using the onmidirectional camera. The corner detection algorithm can be reduced to two steps: line extraction and endpoint collection.
\subsection{Line extraction}
A snapshot from the onmidirectional camera describes a 2D slice of the environment. Points in such a snapshot are specified in the polar coordinate system $(pi, {\theta}i)$, with the origin at the location of the camera. In order to fit a line through the points, a line is expressed in polar coordinates:
\begin{equation}
	x\cos{\alpha} + y\sin{\alpha} = r
\end{equation}
In which $-\pi < \alpha < \pi$ is the angle between the line and the x-axis, $r >= 0$ is the distance of the line to the origin and $(x, y)$ are the Cartesian coordinates of the point lying on the plane. \\ \\
In order to identify the best line segments for the obtained measurements, the Split-and-Merge algorithm is employed.
\subsection{Endpoint collection}

\subsection{Parameters}
The FastSLAM approach estimates the pose of the robots and the features using a particle filter. Each particle represents a possible robots position and its map of the environment. A few parameters of the SLAM algorithm can be tweaked, in order to improve performance of the algorithm:
\begin{enumerate}
\item \textbf{The number of particles} - saved as \verb|PARAMS.NPARTICLES|: this influences the number of particles in the cloud. More particles gives an higher chance of a particle representing the exact position of the robot, but requires more computational power and also introduces more wrong particles.
\item \textbf{The variance of the odometry} - saved as \verb|sigmaX| and \verb|sigmaTH|: this indicates the possible error value for the odometry of the robot. \verb|sigmaX| indicates the error in the speed, while \verb|sigmaTH| indicates the error in rotation. Higher values for these parameters make the size of the particle cloud bigger, and therefore the locations of the walls in the map less certain.
\item \textbf{The variance of the range sensor} - saved as \verb|sigmaR| and \verb|sigmaB|: this indicates the possible error value for the range sensor. Higher values for these parameters make the locations of the walls less certain.
\end{enumerate}
\section{Experiments}
When running the algorithm on the data set provided, it returned the following result:
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\textwidth]{normal.jpg}
\end{figure}
\section{Results}

\section{Conclusion}

\end {document}