\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{xcolor}
% \usepackage[margin=1.7cm]{geometry}
\usepackage{colortbl}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{textcomp}
\newcommand{\todo}[2]{{\color{red}\textbf{Todo voor #1:} #2}}
\begin{document}
\begin{titlepage}
\begin{center}
    \includegraphics[width=\textwidth]{./logo.png}
    \\ [2.5cm]
    \textsc{\Large Autonomous Mobile Robots}
    \\ [0.5cm]
    \textsc{\large Fourth assignment}
    \\ [1cm]
    \hrule
    \vspace{0.3cm}
    \textsc{Particle Filter Based Simultaneous Localization And Mapping}
    \\ [0.3cm]
    \hrule
    \vfill
    \textsc{Ruben Janssen, 10252657 \\ David van Erkelens, 10264019 \\[0.7cm] Department of Computer Science \\ University of Amsterdam \\[0.3cm] \today \\[0.5cm] All code published on}
    \\
    \verb|www.github.com/David1209/AMR/assignment4|
\end{center}
\end{titlepage}
\tableofcontents
\clearpage
\section{Introduction}
In order for an autonomous mobile robot to be able to navigate without a priori knowledge about the map, it has to determine its position whilst building the map. This can be accomplished by using the Simultaneous Localization and Mapping (SLAM) algorithm. One of the variations of SLAM is FastSLAM, a particle filter based algorithm which has a better performance in real time map building than most other variations. FastSLAM scales logarithmically with the number of landmarks in the map, and is therefore much more suitable for real-time applications.

\section{Setup}
A NXT robot has been equipped with an omnidirectional camera. Using this NXT, a dataset of images has been made instead of a live video feed. Eventhough using the dataset is less accurate, the FastSLAM algorithm should be able to compensate for this. A basic environment has been taped out, and from the point of view of the robot some snapshots have been made. 

insert picture of environment here

iets uitgebreider? wat?

\section{FastSLAM}
To implement FastSLAM, a pre-recorded dataset is used. The input of the SLAM algorithm consists of odometry data and observable features. Here, corners are used as landmarks, detected as endpoint segments of lines extracted from data using the onmidirectional camera. The corner detection algorithm can be reduced to two steps: line extraction and endpoint collection.
\subsection{Line extraction}
A snapshot from the onmidirectional camera describes a 2D slice of the environment. Points in such a snapshot are specified in the polar coordinate system $(pi, {\theta}i)$, with the origin at the location of the camera. In order to fit a line through the points, a line is expressed in polar coordinates:
\begin{equation}
	x\cos{\alpha} + y\sin{\alpha} = r
\end{equation}
In which $-\pi < \alpha < \pi$ is the angle between the line and the x-axis, $r >= 0$ is the distance of the line to the origin and $(x, y)$ are the Cartesian coordinates of the point lying on the plane. \\ \\
In order to identify the best line segments for the obtained measurements, the Split-and-Merge algorithm is employed.
\subsection{Endpoint collection}

\subsection{Parameters}
The FastSLAM approach estimates the pose of the robots and the features using a particle filter. Each particle represents a possible robots position and its map of the environment. A few parameters of the SLAM algorithm can be tweaked, in order to improve performance of the algorihm:
\begin{enumerate}
\item \textbf{The number of particles} - saved as \verb|PARAMS.NPARTICLES|:
\item \textbf{The variance of the odometry} - saved as \verb|sigmaX| and \verb|sigmaTH|:
\item \textbf{The variance of the range sensor} - saved as \verb|sigmaR| and \verb|sigmaB|:
\end{enumerate}
\section{Experiments}
When running the algorithm on the data set provided, it returned the following result:
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\textwidth]{normal.jpg}
\end{figure}

\subsection{Results}

\section{Working with a photo dataset}
\subsection{Logging the data}
To build the map of the environment using FastSLAM, the robot keeps track of its movements and environment by reading its encoder and taking photos of its environment respectively. Using this information, FastSLAM is able to locate the robots position and map its environment. In case of a dataset of photos without encoder logs, this becomes troublesome as the movements of the robot are not known exactly. Therefore, these movements have to be estimated by hand.

\subsection{Parameters}
Given that the robot movements are estimated by hand, the uncertainty of the movements increases. As a result, it becomes harder for FastSLAM to localize the robot and map the environment. Because the noise of the movements increase, the robots location is less precise and more uncertain. To compensate for this, the deviations $\sigma_x$ and $\sigma_\theta$ increase, resulting in a less accurate localization of the robot.

In the used photo dataset however, only $\sigma_x$ increases significantly. In this dataset, it is relatively more difficult to estimate the exact distance moved by the robot, than to estimate the difference in orientation. This is because almost all orientation movements by the robot are 45 degrees and can be estimated relative to the environment. To estimate the distance however, it cannot be estimated easily on the surroundings, as all distances are not exactly known.

\subsection{Results}

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{own_normal.jpg}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{own_very_high_sigma.jpg}
\end{figure}

\section{Conclusion}

\end {document}