\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{xcolor}
% \usepackage[margin=1.7cm]{geometry}
\usepackage{colortbl}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{textcomp}
\usepackage{float}
\newcommand{\todo}[2]{{\color{red}\textbf{Todo voor #1:} #2}}
\begin{document}
\begin{titlepage}
\begin{center}
    \includegraphics[width=\textwidth]{./logo.png}
    \\ [2.5cm]
    \textsc{\Large Autonomous Mobile Robots}
    \\ [0.5cm]
    \textsc{\large Fourth assignment}
    \\ [1cm]
    \hrule
    \vspace{0.3cm}
    \textsc{Particle Filter Based Simultaneous Localization And Mapping}
    \\ [0.3cm]
    \hrule
    \vfill
    \textsc{Ruben Janssen, 10252657 \\ David van Erkelens, 10264019 \\[0.7cm] Department of Computer Science \\ University of Amsterdam \\[0.3cm] \today \\[0.5cm] All code published on}
    \\
    \verb|www.github.com/David1209/AMR/assignment4|
\end{center}
\end{titlepage}
\tableofcontents
\clearpage
\section{Introduction}
In order for an autonomous mobile robot to be able to navigate without a priori knowledge about the map, it has to determine its position whilst building the map. This can be accomplished by using the Simultaneous Localization and Mapping (SLAM) algorithm. One of the variations of SLAM is FastSLAM, a particle filter based algorithm which has a better performance in real time map building than most other variations. FastSLAM scales logarithmically with the number of landmarks in the map, and is therefore much more suitable for real-time applications.

\section{Setup}
A NXT robot has been equipped with an omnidirectional camera. Using this NXT, a dataset of images has been made instead of a live video feed. Even though using the dataset is less accurate, the FastSLAM algorithm should be able to compensate for this. A basic environment has been taped out, and from the point of view of the robot some snapshots have been made. 

insert picture of environment here

iets uitgebreider? wat?

\section{FastSLAM}
To implement FastSLAM, a pre-recorded dataset is used. The input of the SLAM algorithm consists of odometry data and observable features. Here, corners are used as landmarks, detected as endpoint segments of lines extracted from data using the omnidirectional camera. The corner detection algorithm can be reduced to two steps: line extraction and endpoint collection.
\subsection{Line extraction}
A snapshot from the omnidirectional camera describes a 2D slice of the environment. Points in such a snapshot are specified in the polar coordinate system $(pi, {\theta}i)$, with the origin at the location of the camera. In order to fit a line through the points, a line is expressed in polar coordinates:
\begin{equation}
	x\cos{\alpha} + y\sin{\alpha} = r
\end{equation}
In which $-\pi < \alpha < \pi$ is the angle between the line and the x-axis, $r >= 0$ is the distance of the line to the origin and $(x, y)$ are the Cartesian coordinates of the point lying on the plane. \\ \\
In order to identify the best line segments for the obtained measurements, the Split-and-Merge algorithm is employed.
\subsection{Endpoint collection}

\subsection{Parameters}
The FastSLAM approach estimates the pose of the robots and the features using a particle filter. Each particle represents a possible robots position and its map of the environment. A few parameters of the SLAM algorithm can be tweaked, in order to improve performance of the algorithm:
\begin{enumerate}
\item \textbf{The number of particles} - saved as \verb|PARAMS.NPARTICLES|: this influences the number of particles in the cloud. More particles gives an higher chance of a particle representing the exact position of the robot, but requires more computational power and also introduces more wrong particles.
\item \textbf{The variance of the odometry} - saved as \verb|sigmaX| and \verb|sigmaTH|: this indicates the possible error value for the odometry of the robot. \verb|sigmaX| indicates the error in the speed, while \verb|sigmaTH| indicates the error in rotation. Higher values for these parameters make the size of the particle cloud bigger, and therefore the locations of the walls in the map less certain.
\item \textbf{The variance of the range sensor} - saved as \verb|sigmaR| and \verb|sigmaB|: this indicates the possible error value for the range sensor. Higher values for these parameters make the locations of the walls less certain.
\end{enumerate}
Results of tweaking the parameters can be seen in figure \ref{fig:tweaks}.
\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{normal.jpg}
		\caption{Normal result}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{1000particles.jpg}
		\caption{More particles}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{sigma_chance.jpg}
		\caption{Using a higher $\sigma$}
	\end{subfigure}
	\caption{Different parameters}
	\label{fig:tweaks}
\end{figure}
\section{Experiments}
When running the algorithm on the data set provided, it returned the following result:
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\textwidth]{normal.jpg}
\end{figure}

\subsection{Results}

\section{Working with a photo dataset}
\subsection{Logging the data}
To build the map of the environment using FastSLAM, the robot keeps track of its movements and environment by reading its encoder and taking photos of its environment respectively. Using this information, FastSLAM is able to locate the robots position and map its environment. In case of a dataset of photos without encoder logs, this becomes troublesome as the movements of the robot are not known exactly. Therefore, these movements have to be estimated by hand.

\subsection{Parameters}
Given that the robot movements are estimated by hand, the uncertainty of the movements increases. As a result, it becomes harder for FastSLAM to localize the robot and map the environment. Because the noise of the movements increase, the robots location is less precise and more uncertain. To compensate for this, the deviations $\sigma_x$ and $\sigma_\theta$ increase, resulting in a less accurate localization of the robot.

In the used photo dataset however, only $\sigma_x$ increases significantly. In this dataset, it is relatively more difficult to estimate the exact distance moved by the robot, than to estimate the difference in orientation. This is because almost all orientation movements by the robot are 45 degrees and can be estimated relative to the environment. To estimate the distance however, it cannot be estimated easily on the surroundings, as all distances are not exactly known.

\subsection{Results}
To examine the influence of the increased $\sigma$ value, two tests were done with different $\sigma_x$ values.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{own_normal.jpg}
	\caption{Low $\sigma_x$}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{own_very_high_sigma.jpg}
	\caption{High $\sigma_x$}
\end{figure}

\section{Conclusion}
In the used photo dataset however, only $\sigma_x$ increases significantly. In this dataset, it is relatively more difficult to estimate the exact distance moved by the robot, than to estimate the difference in orientation. This is because almost all orientation movements by the robot are 45 degrees and can be estimated relative to the environment. To estimate the distance however, it cannot be estimated easily on the surroundings, as all distances are not exactly known.
In the used photo dataset however, only $\sigma_x$ increases significantly. In this dataset, it is relatively more difficult to estimate the exact distance moved by the robot, than to estimate the difference in orientation. This is because almost all orientation movements by the robot are 45 degrees and can be estimated relative to the environment. To estimate the distance however, it cannot be estimated easily on the surroundings, as all distances are not exactly known.
In the used photo dataset however, only $\sigma_x$ increases significantly. In this dataset, it is relatively more difficult to estimate the exact distance moved by the robot, than to estimate the difference in orientation. This is because almost all orientation movements by the robot are 45 degrees and can be estimated relative to the environment. To estimate the distance however, it cannot be estimated easily on the surroundings, as all distances are not exactly known.

\end {document}